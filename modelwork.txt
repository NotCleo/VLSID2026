Summary of doubts
(in regards to this pipeline; make 5cmx5xmx2cm (LxBxH) cardboard boxes, make (generate via a python script) and stick QR codes that encode {product ID, product fragility type} in which we only have 5 product ID's and 2 levels of fragility ; {{1,2,3,4,5}, {low, high}}, QR codes would generally be present in any one of the four corners, then we defined different defect classes as follows; first we can have two kinds {dent, tear} at the positions; {side (border of the top face along the boundary, not to be confused by side faces), face tear (the top face captured)} and also can occur on QR or not, then once we have plenty images all taken in almost same lighting conditions and height, I guess we will try very hard to keep the camera height, angle, and zoom fixed during data collection and similar during deployment, now we have all the images, now i guess we will have to train a raw untrained resnet on our images, or even run transfer learning training in parallel and check which one works, so save the trained model(models) as onnx, then in the bash file as seen for the SDK, replace with our model, and caliberation dataset and it should do the rest i believe....)

1) Our conveyor dimensions are 70mm width and 270mm length
2) We need to ensure our cardboard boxes are 50mmX50mmx20mm (LxBxH) dimensions which we are making of our own
3) we aim to have defect detection, then a classification, within it there are sub categories 
4) We have QR codes on the top face which will encode product type (like level of fragility)and product ID (to help us flag it)
5) We ideally need to place the camera at 15 cm mark (putting 2 conveyor belts back to back gives us 54cm length)
6) Once camera captures the first box, it has roughly 50-15cm = 35cm - 2cm buffer = 33cm length to travel while inference occurs 
7) We will have a linear velocity of the belt somehow to be 5cm/s so we will get roughly 3-6 seconds to make an inference and make the servo active based on the decision

what problems we faced while discussing, 

1) what if QR code gets damaged, how will we proceed? whats the fallback/ way to flag it?

2) What should our dataset size be, for non defective and defective so that resnet can decently learn a variety of features and classify accurately

3) How big should the dataset of the "non defective" class be, because after all the boxes are of same face dimensions, under same lighting conditons, only the orientation (along z axis) is different and non defective in itself is straightforward right?, but regarding the defective class we have the following classes {dents, tears} and of two forms {side, face}, making around {side dent only, side tear only, face dent only, face tear only, side tear face dent,...} so how do i approach about this? and i have not mentioned yet, we need to consider QR damage, which too would come in two forms; tear on QR and dent on QR. so in totality we will have these many labels, let QR be on on any one of the corner part of the box, so many labels is that??

      Side dent (QR not affected) 
      Side tear (QR not affected)
      Face dent (QR not affected)
      Face tear (QR not affected)
      QR dent only
      QR tear only
      Side dent + Face dent (QR not affected)
      Side tear + Face tear (QR not affected)
      Side dent + Face tear (QR not affected)
      Side tear + Face dent (QR not affected)
      Side dent + QR dent
      Side tear + QR tear
      Face dent + QR dent
      Face tear + QR tear
      Side dent + QR tear (QR affected by tear, side by dent)
      Side tear + QR dent (QR affected by dent, side by tear)
      Face dent + QR tear
      Face tear + QR dent
      Side dent + Face dent + QR dent
      Side tear + Face tear + QR tear
      Side dent + Face tear + QR dent
      Side dent + Face tear + QR tear
      Side tear + Face dent + QR dent
      Side tear + Face dent + QR tear
      Side dent affecting QR + Face tear not affecting QR
      Side tear affecting QR + Face dent not affecting QR


NOTE : Regarding QR codes, we need to figure out on what our fallback is, I suppose it should be that whether QR was found or not we can set that as a flag in itself
NOTE : Regarding QR Codes, we need to set up what amount of damage is acceptable even on QR codes 

4) How to introduce a mapping between degree of damage and product fragility information? without explicitly having different labels while training and physcially creating it on the boxes?

take image -> process to have just top face cardboard -> model checks if damage is there or not, if damage found, what kind/kinds of damage?, [then what level of damage or degree of damages; ignore this] -> give a score -> compare with threshold for different kind/kinds and apply the interrupt to reject via the servo

We originally wanted to try the very complicated mapping between "what kinds of damage on the box" v/s "how much % of damage in each kind" v/s "what's the product's fragility status inside the box"!!!

Another serious doubt; say we are making a dataset from scratch and i use my phone camera to take pictures of cardboard boxes, while doing so i set some zoom like 2x to try to capture majority of the cardboard face in the frame, then i fixed the phone at 2m lets say and kept taking pictures then train the model with these images, then when i give it an image which was taken in these conditions, where i didnt do zoom but 1x and physically moved the camera closer to mimic "try to capture majority of the cardboard face in the frame" so how does that affect the inference? so regarding this;

    So your model trained on the “zoomed” images will see different geometric relationships (less perspective, flatter surface), whereas the test images shot closer will have different spatial distortions — even if both occupy same area in frame.
    
    I guess we will have to "Keep camera height, angle, and zoom fixed during data collection and deployment."
    
    For small zoom changes (1× ↔ 1.2×) and distances ~10–30 cm, CNNs usually tolerate it fine — especially if you include augmentation (random scaling, cropping).
    But a change from 2× optical to 1× close-up can be equivalent to 20–30 % difference in perspective distortion, which is noticeable unless the network saw both cases in training.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Now comes the parameters that are fixed and variable, which I think would affect how we construct the dataset itself 

Fixed parameters/Conditions : 

Speed at which the conveyor moves, we will add a motor driver (need to do some wire trimings lmao)
Our cardboard box dimension is fixed
Our camera's height and zoom, in the sense the zoom and height at which camera is placed is fixed 
The combination of defects we produce on the training and validation dataset
The orientation in which the boxes flow on the belt, all of them would be ideally be perfectly aligned to be straight always

Variable Parameters/Condtions :

just the orientation of how the boxes are along the z -axis (like rotation)

For now we are time constrained so we are not feeding degree of defect classes to the model just defect type classes, so how do we proceed?

------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Primary Task : Post Packaging Quality Control (Rejection based on degree of damage)

How are we approaching it?

if i have a custom dataset of just top view images of cardboard boxes
 
1) each image has one cardboard box from top view (90 degree) top down
2) cardboard may or may not be in same orientation in terms of rotation
3) cardboard boxes are off same dimensions (in terms of length and breadth of top face when observed from top)
4) the face of box which the camera captures will not have some text/printed illustration on it

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The task : (According to the dataset which we plane to make)

(a) detect the cardboard box in the image frame captured
(b) generate a crop of that cardboard box standalone 
(c) run a model to detect defects (tears, dents/depression in surface of the top face that's being captured) 
(d) after any of the following defect is detected, it should give a "score/decision using which we reject or not, (basically classify presence of defect, but idk how will we introduct "degree of defect" in this work, and use it too, but again for now focus on detecting damage)

I feel atleast till step b we can pull it off with some opencv edge detection and thresholding

then for step c we need to run the model!!

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

Continued task : Once primary task dataset (as described above but with a QR code attached) is created for the task described below, we will perform:

(a) The face of the cardboard box captured will have a QR/Bardcode which encodes information regarding the inside contents such as degree of fragility of product and product ID (i have generated the QR's and will stick it in the cardboard boxes we will make)

(b) Essentially, we need to perform; f : (types of damages found, degree of damage for each kind of damage, degree of fragility of product) -> (rejection decision) but like before i think we will first need to get something simpler done first, f : (types of damages found, degree of fragility of product) -> (rejection decision)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

We have a vectorblox SDK which performs PTQ of fp16-> int8 on popular TF models and in that we want to proceed with ONNX models that will run in our Microchip FPGA

Source | Tutorial | Input (H,W,C) | V1000 FPS | Task | Metric | TFLITE | VNNX
onnx |onnx_resnet18-v1| [224, 224, 3] |32.7| classification |Top1 |69.3| 68.8
onnx |onnx_resnet34-v1 |[224, 224, 3] |17.8 |classification |Top1 |72.6 |72.2
onnx |onnx_squeezenet1.1| [224, 224, 3]| 132.6| classification| Top1 |54.0 |54.8
onnx |scrfd_500m_bnkps |[288, 512, 3] |85.8 |face detection|x |x |x
onnx |yolov7 |[640, 640, 3] |1.0 |object detection |mAP⁵⁰⁻⁹⁵ |x|x
onnx |yolov9-s| [640, 640, 3] |3.9 | object detection| mAP⁵⁰⁻⁹⁵| x|x
onnx |yolov9-m| [640, 640, 3] |1.2 |object detection |mAP⁵⁰⁻⁹⁵ |x|x

Here's an example from the SDK, does for yolov7;

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

##########################################################
# _ __ __ ____ __ #
# | | / /__ _____/ /_____ _____/ __ )/ /___ _ __ #
# | | / / _ \/ ___/ __/ __ \/ ___/ __ / / __ \| |/_/ #
# | |/ / __/ /__/ /_/ /_/ / / / /_/ / / /_/ /> < #
# |___/\___/\___/\__/\____/_/ /_____/_/\____/_/|_| #
# #
# https://github.com/Microchip-Vectorblox/VectorBlox-SDK #
# v2.0 #
# #
##########################################################

set -e
echo "Checking and activating VBX Python Environment..."
if [ -z $VBX_SDK ]; then
echo "\$VBX_SDK not set. Please run 'source setup_vars.sh' from the SDK's root folder" && exit 1
fi
source $VBX_SDK/vbx_env/bin/activate

<we need to set up calibration data file for our dataset here, by using the generate_npy python script>
echo "Checking for Numpy calibration data file..."
if [ ! -f $VBX_SDK/tutorials/imagenetv2_rgb_norm_20x224x224x3.npy ]; then
generate_npy $VBX_SDK/tutorials/imagenetv2_rgb_20x224x224x3.npy -o $VBX_SDK/tutorials/imagenetv2_rgb_norm_20x224x224x3.npy -s 224 224 --norm
fi

<we need to then give our onnx resnet here, either the untrained one trained from scratch or the transfer learning learnt model, we will test and take the best onnx>
echo "Checking for onnx_resnet18-v1 files..."
if [ ! -f onnx_resnet18-v1.tflite ]; then
# model details @ https://github.com/onnx/models/tree/main/validated/vision/classification/resnet
wget -q --no-check-certificate https://media.githubusercontent.com/media/onnx/models/main/validated/vision/classification/resnet/model/resnet18-v1-7.onnx
fi

if [ ! -f onnx_resnet18-v1.tflite ]; then
echo "Running ONNX2TF..."
onnx2tf -cind data $VBX_SDK/tutorials/imagenetv2_rgb_norm_20x224x224x3.npy [[[[0.485,0.456,0.406]]]] [[[[0.229,0.224,0.225]]]] \
-i resnet18-v1-7.onnx \
--output_signaturedefs \
--output_integer_quantized_tflite
cp saved_model/resnet18-v1-7_full_integer_quant.tflite onnx_resnet18-v1.tflite
fi
if [ -f onnx_resnet18-v1.tflite ]; then
tflite_preprocess onnx_resnet18-v1.tflite --mean 123.675 116.28 103.53 --scale 58.4 57.1 57.38
fi

if [ -f onnx_resnet18-v1.pre.tflite ]; then
echo "Generating VNNX for V1000 configuration..."
vnnx_compile -c V1000 -t onnx_resnet18-v1.pre.tflite -o onnx_resnet18-v1.vnnx
fi

if [ -f onnx_resnet18-v1.vnnx ]; then
echo "Running Simulation..."
python $VBX_SDK/example/python/classifier.py onnx_resnet18-v1.vnnx $VBX_SDK/tutorials/test_images/oreo.jpg
echo "C Simulation Command:"
echo '$VBX_SDK/example/sim-c/sim-run-model onnx_resnet18-v1.vnnx $VBX_SDK/tutorials/test_images/oreo.jpg CLASSIFY'
fi

deactivate

----------------------------------------------------------------------------------------------------------------------------------------------------------------------

In short we have the following actions happening,

SDK Downloads and converts a trained YOLOv7 model to ONNX → TensorFlow → TFLite → VNNX.

So the following doubts;

1) What model fits our specific task the best? or do you suggest a different model altogether?
2) how to generate NumPy calibration data file for quantization, which is custom to our dataset and not ImageNet/Coco dataset on which the models have been trained 
3) I think we need to first train yolo/resnet with our dataset and save it as a onnx, then apply the bash file, updated wget with our trained model, and then update the calibration dataset (.npy file) tailored for our dataset 
4) ..... done right?



How to generate the vectorblox IP (V250!)
        1) ssh into our container
        2) at /home/joeld, the VectorBlox SDK has already been cloned 
        3) cd into /home/joeld/VectorBlox-SDK/ and run "bash install_dependencies.sh"
        4) In the same directory run "source setup_vars.sh"
        5) Confirm whether the virtual environment has been configured and activated
        6) cd $VBX_SDK/tutorials/{network_source}/{network} (for example; cd $VBX_SDK/tutorials/onnx/yolov7)
        7) bash {network}.sh (for example; bash yolov7.sh) 
        8) before running (7) we need to ensure our build flags is set to V250 and not V1000
The VBXIP gets compiled and creates .vnnx, .hex and a lot many files

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Now I believe I need to do the following;

1) Find the .py file for the model, but before that what model do i choose see the table .....?? and match it to our task
answer) I believe, It is ResNet-34 and here are the specifications

                         Source | Tutorial | Input (H,W,C) | V1000 FPS | Task | Metric | TFLITE | VNNX
                         onnx |onnx_resnet34-v1 | [224, 224, 3] |17.8 | classification  | Top1  |72.6 |72.2

2) Train the .py of just the bare ResNet untrained model with the dataset we make  
3) Then save the trained model as onnx
4) Take a subset of the dataset and run the calibration python script from here - https://github.com/Microchip-Vectorblox/VectorBlox-SDK/blob/master/python/vbx/vbx/generate/generate_npy.py
5) proceed with the VBX bash file where ill have to edit in the paths/files with the calibrated numpy file and the model trained on our kaggle dataset, then it will auto proceed with the onnx-tf-tflite-vnnx conversion i hope
6) We need to fix up our dataset by having both defective and normal in order to classify (but again how many of what kinds?)

Important reference files : 
1) https://github.com/Microchip-Vectorblox/VectorBlox-SDK/blob/master/python/vbx/vbx/generate/generate_npy.py
2) https://github.com/Microchip-Vectorblox/VectorBlox-SDK/blob/master/tutorials/onnx/onnx_resnet34-v1/onnx_resnet34-v1.sh
 
