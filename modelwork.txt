Summary of doubts
(in regards to this pipeline; make 5cmx5xmx2cm (LxBxH) cardboard boxes, make (generate via a python script) and stick QR codes that encode {product ID, product fragility type} in which we only have 5 product ID's and 2 levels of fragility ; {{1,2,3,4,5}, {low, high}}, QR codes would generally be present in any one of the four corners, then we defined different defect 


so our conveyor dimensions are 70mm width and 270mm length
we need to ensure our cardboard boxes are 50mmX50mmx20mm dimensions which we are making our own
we aim to have defect detection, then a classification, within it there are 3 sub categories
the three categories are dent, tear, dent+tear
once we are able to classify a defect, we then need to find the degree of defect
we have QR codes on the top face which will encode product type (like level of fragility)and product ID (to help us flag it)
we ideally need to place the camera at 15 cm mark (putting 2 conveyor belts back to back gives us 54cm length)
meaning we can place 2-3 of our boxes (each being 5cm long so keeping a separation of 10 cm we get 10+5+10+5+10+5 read this right to left to get what i mean :), but for demo sakes, we will place only one box per pass, and let it complete the entire stretch and show that yes our model and demo works then to step it up we will 
then once camera captures the first box, it has roughly 50-15cm = 35cm - 2cm buffer = 33cm length to travel while inference occurs 
we will have a linear velocity of the belt somehow to be 5cm/s so we will get roughly 3-5 seconds to make an inference and make the ultrasonic and servo active

what problems we faced while discussing, 

  what if QR code gets damaged, how will we proceed? whats the fallback/ way to flag it?

  so what should our dataset size be, for non defective and defective so that resnet can decently learn a variety of features and classify accurately

  can i train resnet model (untrained) with my dataset, which I will make by taking pictures of cardboard boxes and doing different kinds and amount of damage, mostly would involve  a side dent, a face dent (remember the camera always faces from top to bottom on the top face, where the qr code is also stuck), a side+face dent, a side tear, a face tear, a face+side tear, a side dent+face tear, a face tear + face dent, ..... , so what size of dataset?! 

  how big should the dataset of the "non defective" class be, because after all the boxes are of same size and non defective in itself is straightforward right?, but regarding the defective class we have the following classes {dents, tears} and of two forms {side, face}, making around {side dent only, side tear only, face dent only, face tear only, side tear face dent, face tear side dent, side dent side tear, face dent face tear} so how do i approach about this? and i forgot to consider QR damage, which too would come in two forms; tear on QR and dent on QR. so in totality we will have these many labels, let QR be on bottom left part of the box, so many labels is that??

      Side dent (QR not affected) 
      Side tear (QR not affected)
      Face dent (QR not affected)
      Face tear (QR not affected)
      QR dent only
      QR tear only
      Side dent + Face dent (QR not affected)
      Side tear + Face tear (QR not affected)
      Side dent + Face tear (QR not affected)
      Side tear + Face dent (QR not affected)
      Side dent + QR dent
      Side tear + QR tear
      Face dent + QR dent
      Face tear + QR tear
      Side dent + QR tear (QR affected by tear, side by dent)
      Side tear + QR dent (QR affected by dent, side by tear)
      Face dent + QR tear
      Face tear + QR dent
      Side dent + Face dent + QR dent
      Side tear + Face tear + QR tear
      Side dent + Face tear + QR dent
      Side dent + Face tear + QR tear
      Side tear + Face dent + QR dent
      Side tear + Face dent + QR tear
      Side dent affecting QR + Face tear not affecting QR
      Side tear affecting QR + Face dent not affecting QR


I have generated QR for these tags; a QR code that opens a photo (jpeg) showing "product ID and product type" so i got 10 QR codes generated, like product ID = 1,2,3,4,5 ... and product type = " extremely fragile, partially fragile, partially solid, extremely solid"

Now we need to clarify that while constructing the dataset we will i think need to take a bunch of boxes and add QR that i generated above and stick it on the cardboard, then try taking photos of it from perfect 90 degree top view, and here's the issue, will the effect by zooming I set in my phone which ill use to create the dataset and by physically trying to "achieve" zoom by bringing the phone closer, so how does that affect the training and accuracy and robustness?

Ideally the camera clicks a frame, then comes another query regarding the zoom, but let's just say that we have applied some zoom to ensure we are not captuing too much and just enough to have the box face occupy most of the frame, but for a safe side we will need to run some preprocessing where it crops the face of the cardboard (in any condition; with/without defect, remember defect can be severe, like a massive tear, so such edge cases how will it handle, like suppose it just made a crop incorrectly so model will train very weakly, and it becomes very randomized training), but again let's say that our preprocessing somehow captures the box face well and passes it to the bare resnet to train on, how many images of each category should we give in order to ensure the model learns the feature deep enough to be accurate, then let's say we have around 2-4 seconds between the image captured, processing, inference and the decision to reject based on defect or not, but during the processing, there's another layer of complexity, which is, finding the degree of defect, we plan to have two-three degree of defect; {low, high} or {low, medium, high}, i'll take just two levels for simplicity, that's around 116 cases!! so now how many images and given we create 3 lighting conditions {low, medium, high} (now can we just change the exposure of the camera or a physical light source has to be used to create different lighting conditions? or will adding an uniform light source which is constantly on help? now i have a doubt, will adding the degree of damage label affect how my inference works, like i essentially need this;

take image -> process to have just top face cardboard -> model checks if damage is there or not, if damage found, what kind/kinds of damage?, [then what level of damage or degree of damages; ignore this] -> give a score -> compare with threshold for different kind/kinds and apply the interrupt to reject via the servo

very complicated mapping between "what kinds of damage on the box" v/s "how much % of damage in each kind" v/s "what's the product's fragility status inside the box"!!!

Now comes the parameters that are fixed and variable, which I think would affect how we construct the dataset itself 

Fixed parameters/Conditions : 

Speed at which the conveyor moves, we will add a motor driver (need to do some wire trimings lmao)
Our cardboard box dimension is fixed
Our camera's height and zoom, in the sense the zoom and height at which camera is placed is fixed 
The combination of defects we produce on the training and validation dataset
The orientation in which the boxes flow on the belt, all of them would be ideally be perfectly aligned to be straight always

Variable Parameters/Condtions :

just the orientation of how the boxes are along the z -axis (like rotation)

For now we are time constrained so we are not feeding degree of defect classes to the model just defect type classes, so how do we proceed?

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

Primary Task : Post Packaging Quality Control (Rejection based on degree of damage)

How are we approaching it?

if i have a custom dataset of just top view images of cardboard boxes
 
1) each image has one cardboard box from top view (90 degree) top down
2) cardboard may or may not be in same orientation in terms of rotation
3) cardboard boxes are off same dimensions (in terms of length and breadth of top face when observed from top)
4) the face of box which the camera captures will not have some text/printed illustration on it

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The task : (According to the dataset which we plane to make)

(a) detect the cardboard box in the image frame captured
(b) generate a crop of that cardboard box standalone 
(c) run a model to detect defects (tears, dents/depression in surface of the top face that's being captured) 
(d) after any of the following defect is detected, it should give a "score/decision using which we reject or not, (basically classify presence of defect, but idk how will we introduct "degree of defect" in this work, and use it too, but again for now focus on detecting damage)

I feel atleast till step b we can pull it off with some opencv edge detection and thresholding

then for step c we need to run the model!!

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

Continued task : Once primary task dataset (as described above but with a QR code attached) is created for the task described below, we will perform:

(a) The face of the cardboard box captured will have a QR/Bardcode which encodes information regarding the inside contents such as degree of fragility of product and product ID (i have generated the QR's and will stick it in the cardboard boxes we will make)

(b) Essentially, we need to perform; f : (types of damages found, degree of damage for each kind of damage, degree of fragility of product) -> (rejection decision) but like before i think we will first need to get something simpler done first, f : (types of damages found, degree of fragility of product) -> (rejection decision)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

We have a vectorblox SDK which performs PTQ of fp16-> int8 on popular TF models and in that we want to proceed with ONNX models that will run in our Microchip FPGA

Source | Tutorial | Input (H,W,C) | V1000 FPS | Task | Metric | TFLITE | VNNX
onnx |onnx_resnet18-v1| [224, 224, 3] |32.7| classification |Top1 |69.3| 68.8
onnx |onnx_resnet34-v1 |[224, 224, 3] |17.8 |classification |Top1 |72.6 |72.2
onnx |onnx_squeezenet1.1| [224, 224, 3]| 132.6| classification| Top1 |54.0 |54.8
onnx |scrfd_500m_bnkps |[288, 512, 3] |85.8 |face detection|x |x |x
onnx |yolov7 |[640, 640, 3] |1.0 |object detection |mAP⁵⁰⁻⁹⁵ |x|x
onnx |yolov9-s| [640, 640, 3] |3.9 | object detection| mAP⁵⁰⁻⁹⁵| x|x
onnx |yolov9-m| [640, 640, 3] |1.2 |object detection |mAP⁵⁰⁻⁹⁵ |x|x

Here's an example from the SDK, does for yolov7;

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

##########################################################
# _ __ __ ____ __ #
# | | / /__ _____/ /_____ _____/ __ )/ /___ _ __ #
# | | / / _ \/ ___/ __/ __ \/ ___/ __ / / __ \| |/_/ #
# | |/ / __/ /__/ /_/ /_/ / / / /_/ / / /_/ /> < #
# |___/\___/\___/\__/\____/_/ /_____/_/\____/_/|_| #
# #
# https://github.com/Microchip-Vectorblox/VectorBlox-SDK #
# v2.0 #
# #
##########################################################

set -e
echo "Checking and activating VBX Python Environment..."
if [ -z $VBX_SDK ]; then
echo "\$VBX_SDK not set. Please run 'source setup_vars.sh' from the SDK's root folder" && exit 1
fi
source $VBX_SDK/vbx_env/bin/activate

echo "Checking for Numpy calibration data file..."
if [ ! -f $VBX_SDK/tutorials/imagenetv2_rgb_norm_20x224x224x3.npy ]; then
generate_npy $VBX_SDK/tutorials/imagenetv2_rgb_20x224x224x3.npy -o $VBX_SDK/tutorials/imagenetv2_rgb_norm_20x224x224x3.npy -s 224 224 --norm
fi

echo "Checking for onnx_resnet18-v1 files..."
if [ ! -f onnx_resnet18-v1.tflite ]; then
# model details @ https://github.com/onnx/models/tree/main/validated/vision/classification/resnet
wget -q --no-check-certificate https://media.githubusercontent.com/media/onnx/models/main/validated/vision/classification/resnet/model/resnet18-v1-7.onnx
fi

if [ ! -f onnx_resnet18-v1.tflite ]; then
echo "Running ONNX2TF..."
onnx2tf -cind data $VBX_SDK/tutorials/imagenetv2_rgb_norm_20x224x224x3.npy [[[[0.485,0.456,0.406]]]] [[[[0.229,0.224,0.225]]]] \
-i resnet18-v1-7.onnx \
--output_signaturedefs \
--output_integer_quantized_tflite
cp saved_model/resnet18-v1-7_full_integer_quant.tflite onnx_resnet18-v1.tflite
fi
if [ -f onnx_resnet18-v1.tflite ]; then
tflite_preprocess onnx_resnet18-v1.tflite --mean 123.675 116.28 103.53 --scale 58.4 57.1 57.38
fi

if [ -f onnx_resnet18-v1.pre.tflite ]; then
echo "Generating VNNX for V1000 configuration..."
vnnx_compile -c V1000 -t onnx_resnet18-v1.pre.tflite -o onnx_resnet18-v1.vnnx
fi

if [ -f onnx_resnet18-v1.vnnx ]; then
echo "Running Simulation..."
python $VBX_SDK/example/python/classifier.py onnx_resnet18-v1.vnnx $VBX_SDK/tutorials/test_images/oreo.jpg
echo "C Simulation Command:"
echo '$VBX_SDK/example/sim-c/sim-run-model onnx_resnet18-v1.vnnx $VBX_SDK/tutorials/test_images/oreo.jpg CLASSIFY'
fi

deactivate

----------------------------------------------------------------------------------------------------------------------------------------------------------------------

In short we have the following actions happening,

SDK Downloads and converts a trained YOLOv7 model to ONNX → TensorFlow → TFLite → VNNX.

So the following doubts;

1) What model fits our specific task the best? or do you suggest a different model altogether?
2) how to generate NumPy calibration data file for quantization, which is custom to our dataset and not ImageNet/Coco dataset on which the models have been trained 
3) I think we need to first train yolo/resnet with our dataset and save it as a onnx, then apply the bash file, updated wget with our trained model, and then update the calibration dataset (.npy file) tailored for our dataset 
4) ..... done right?



How to generate the vectorblox IP (V250!)
        1) ssh into our container
        2) at /home/joeld, the VectorBlox SDK has already been cloned 
        3) cd into /home/joeld/VectorBlox-SDK/ and run "bash install_dependencies.sh"
        4) In the same directory run "source setup_vars.sh"
        5) Confirm whether the virtual environment has been configured and activated
        6) cd $VBX_SDK/tutorials/{network_source}/{network} (for example; cd $VBX_SDK/tutorials/onnx/yolov7)
        7) bash {network}.sh (for example; bash yolov7.sh) 
        8) before running (7) we need to ensure our build flags is set to V250 and not V1000
The VBXIP gets compiled and creates .vnnx, .hex and a lot many files

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Now I believe I need to do the following;

1) Find the .py file for the model, but before that what model do i choose see the table .....?? and match it to our task
answer) I believe, It is ResNet-34 and here are the specifications

                         Source | Tutorial | Input (H,W,C) | V1000 FPS | Task | Metric | TFLITE | VNNX
                         onnx |onnx_resnet34-v1 | [224, 224, 3] |17.8 | classification  | Top1  |72.6 |72.2

2) Train the .py of just the bare ResNet untrained model with the dataset we make  
3) Then save the trained model as onnx
4) Take a subset of the dataset and run the calibration python script from here - https://github.com/Microchip-Vectorblox/VectorBlox-SDK/blob/master/python/vbx/vbx/generate/generate_npy.py
5) proceed with the VBX bash file where ill have to edit in the paths/files with the calibrated numpy file and the model trained on our kaggle dataset, then it will auto proceed with the onnx-tf-tflite-vnnx conversion i hope
6) We need to fix up our dataset by having both defective and normal in order to classify (but again how many of what kinds?)

Important reference files : 
1) https://github.com/Microchip-Vectorblox/VectorBlox-SDK/blob/master/python/vbx/vbx/generate/generate_npy.py
2) https://github.com/Microchip-Vectorblox/VectorBlox-SDK/blob/master/tutorials/onnx/onnx_resnet34-v1/onnx_resnet34-v1.sh
 
